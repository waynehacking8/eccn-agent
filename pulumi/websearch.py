#!/usr/bin/env python3
"""
WebSearch æ•´åˆæ¨¡çµ„
æä¾›ECCNç›¸é—œçš„ç¶²è·¯æœç´¢åŠŸèƒ½
åŒ…å«å®˜æ–¹æ³•è¦ã€å•†æ¥­è³‡æ–™åº«ã€æŠ€è¡“æ–‡æª”æœç´¢
"""

import json
import requests
import re
import time
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime
from urllib.parse import quote, urljoin
import html

class ECCNWebSearcher:
    """ECCNç¶²è·¯æœç´¢å™¨"""
    
    def __init__(self, logger: logging.Logger = None):
        self.logger = logger or logging.getLogger(__name__)
        
        # æœç´¢é…ç½®
        self.timeout = 30
        self.max_results_per_query = 10
        self.rate_limit_delay = 1
        
        # ECCN ç›¸é—œé—œéµå­—å’Œæ¨¡å¼
        self.eccn_pattern = re.compile(r'\b(EAR99|[0-9][A-Z][0-9]{3}(?:\.[a-z](?:\.[0-9]+)?)?)\b', re.IGNORECASE)
        
        # å®˜æ–¹å’Œæ¬Šå¨ä¾†æºç¶²åŸŸ
        self.authoritative_domains = {
            'bis.doc.gov': {'weight': 10, 'type': 'government'},
            'export.gov': {'weight': 10, 'type': 'government'},
            'trade.gov': {'weight': 9, 'type': 'government'},
            'census.gov': {'weight': 8, 'type': 'government'},
            'cbp.gov': {'weight': 8, 'type': 'government'},
            'mouser.com': {'weight': 7, 'type': 'distributor'},
            'digikey.com': {'weight': 7, 'type': 'distributor'},
            'advantech.com': {'weight': 6, 'type': 'manufacturer'},
            'cisco.com': {'weight': 6, 'type': 'manufacturer'},
            'dell.com': {'weight': 6, 'type': 'manufacturer'},
            'hp.com': {'weight': 6, 'type': 'manufacturer'},
            'intel.com': {'weight': 6, 'type': 'manufacturer'}
        }
        
        # æœç´¢æŸ¥è©¢æ¨¡æ¿
        self.query_templates = {
            'official_eccn': '"{product_model}" ECCN site:bis.doc.gov OR site:export.gov',
            'distributor_eccn': '"{product_model}" ECCN site:mouser.com OR site:digikey.com',
            'manufacturer_eccn': '"{product_model}" "export control" OR "ECCN" site:{manufacturer_domain}',
            'general_eccn': '"{product_model}" ECCN "export control classification"',
            'ccl_search': '"{product_model}" "commerce control list" CCL',
            'regulatory_search': '"{product_model}" "export administration regulations" EAR',
            'specification_search': '"{product_model}" specifications datasheet ECCN'
        }

    def search_eccn_information(self, product_model: str, manufacturer: str = None) -> List[Dict]:
        """
        æœç´¢ç”¢å“çš„ECCNç›¸é—œè³‡è¨Š
        
        Args:
            product_model: ç”¢å“å‹è™Ÿ
            manufacturer: è£½é€ å•†åç¨± (å¯é¸)
            
        Returns:
            æœç´¢çµæœæ¸…å–®
        """
        try:
            self.logger.info(f"ğŸ” é–‹å§‹WebSearch ECCNæŸ¥è©¢: {product_model}")
            
            all_results = []
            
            # 1. å®˜æ–¹æ”¿åºœä¾†æºæœç´¢
            official_results = self._search_official_sources(product_model)
            all_results.extend(official_results)
            
            # 2. ç¶“éŠ·å•†ç¶²ç«™æœç´¢
            distributor_results = self._search_distributor_sources(product_model)
            all_results.extend(distributor_results)
            
            # 3. è£½é€ å•†ç¶²ç«™æœç´¢
            if manufacturer:
                manufacturer_results = self._search_manufacturer_sources(product_model, manufacturer)
                all_results.extend(manufacturer_results)
            
            # 4. ä¸€èˆ¬ECCNè³‡æ–™åº«æœç´¢
            general_results = self._search_general_sources(product_model)
            all_results.extend(general_results)
            
            # 5. åˆ†æå’Œæ’åºçµæœ
            processed_results = self._process_search_results(all_results, product_model)
            
            # è¨ˆç®—åŒ…å«PENDING_VERIFICATIONçš„çµæœæ•¸é‡
            pending_count = sum(1 for r in processed_results if r.get('eccn_code') == 'PENDING_VERIFICATION')
            confirmed_count = len(processed_results) - pending_count
            
            self.logger.info(f"âœ… WebSearchå®Œæˆï¼Œæ‰¾åˆ° {confirmed_count} å€‹ç¢ºèªçµæœï¼Œ{pending_count} å€‹å¾…é©—è­‰çµæœ")
            return processed_results
            
        except Exception as e:
            self.logger.error(f"âŒ WebSearchå¤±æ•—: {str(e)}")
            return []

    def _search_official_sources(self, product_model: str) -> List[Dict]:
        """æœç´¢å®˜æ–¹æ”¿åºœä¾†æº"""
        
        self.logger.info("ğŸ›ï¸ æœç´¢å®˜æ–¹æ”¿åºœä¾†æº")
        results = []
        
        queries = [
            self.query_templates['official_eccn'].format(product_model=product_model),
            f'"{product_model}" "commerce control list"',
            f'"{product_model}" "export administration regulations"'
        ]
        
        for query in queries:
            try:
                search_results = self._perform_web_search(query, source_type='official')
                results.extend(search_results)
                time.sleep(self.rate_limit_delay)
            except Exception as e:
                self.logger.warning(f"å®˜æ–¹ä¾†æºæœç´¢å¤±æ•—: {query} - {str(e)}")
        
        return results

    def _search_distributor_sources(self, product_model: str) -> List[Dict]:
        """æœç´¢ç¶“éŠ·å•†ä¾†æº"""
        
        self.logger.info("ğŸª æœç´¢ç¶“éŠ·å•†ä¾†æº")
        results = []
        
        queries = [
            self.query_templates['distributor_eccn'].format(product_model=product_model),
            f'"{product_model}" ECCN site:arrow.com',
            f'"{product_model}" "export control" site:farnell.com'
        ]
        
        for query in queries:
            try:
                search_results = self._perform_web_search(query, source_type='distributor')
                results.extend(search_results)
                time.sleep(self.rate_limit_delay)
            except Exception as e:
                self.logger.warning(f"ç¶“éŠ·å•†ä¾†æºæœç´¢å¤±æ•—: {query} - {str(e)}")
        
        return results

    def _search_manufacturer_sources(self, product_model: str, manufacturer: str) -> List[Dict]:
        """æœç´¢è£½é€ å•†ä¾†æº"""
        
        self.logger.info(f"ğŸ­ æœç´¢è£½é€ å•†ä¾†æº: {manufacturer}")
        results = []
        
        # æ¨æ–·è£½é€ å•†ç¶²åŸŸ
        manufacturer_domain = self._infer_manufacturer_domain(manufacturer)
        
        if manufacturer_domain:
            query = self.query_templates['manufacturer_eccn'].format(
                product_model=product_model,
                manufacturer_domain=manufacturer_domain
            )
            
            try:
                search_results = self._perform_web_search(query, source_type='manufacturer')
                results.extend(search_results)
            except Exception as e:
                self.logger.warning(f"è£½é€ å•†ä¾†æºæœç´¢å¤±æ•—: {manufacturer} - {str(e)}")
        
        return results

    def _search_general_sources(self, product_model: str) -> List[Dict]:
        """æœç´¢ä¸€èˆ¬ä¾†æº"""
        
        self.logger.info("ğŸŒ æœç´¢ä¸€èˆ¬ä¾†æº")
        results = []
        
        queries = [
            self.query_templates['general_eccn'].format(product_model=product_model),
            self.query_templates['ccl_search'].format(product_model=product_model),
            self.query_templates['specification_search'].format(product_model=product_model)
        ]
        
        for query in queries:
            try:
                search_results = self._perform_web_search(query, source_type='general')
                results.extend(search_results)
                time.sleep(self.rate_limit_delay)
            except Exception as e:
                self.logger.warning(f"ä¸€èˆ¬ä¾†æºæœç´¢å¤±æ•—: {query} - {str(e)}")
        
        return results

    def _perform_web_search(self, query: str, source_type: str = 'general') -> List[Dict]:
        """
        åŸ·è¡Œå¯¦éš›çš„ç¶²è·¯æœç´¢
        ä½¿ç”¨DuckDuckGoé€²è¡ŒçœŸå¯¦æœç´¢ï¼Œé¿å…æ¨¡æ“¬çµæœå¹²æ“¾
        """
        
        self.logger.info(f"ğŸ” åŸ·è¡ŒçœŸå¯¦WebSearchæŸ¥è©¢: {query}")
        
        try:
            # ä½¿ç”¨ DuckDuckGo HTML æœç´¢ (ç„¡éœ€APIé‡‘é‘°)
            return self._duckduckgo_search(query)
        except Exception as e:
            self.logger.error(f"âŒ WebSearchæŸ¥è©¢å¤±æ•—: {str(e)}")
            return []


    def _process_search_results(self, raw_results: List[Dict], product_model: str) -> List[Dict]:
        """è™•ç†å’Œåˆ†ææœç´¢çµæœ"""
        
        processed_results = []
        
        for result in raw_results:
            try:
                # æå–ECCNè³‡è¨Š
                eccn_info = self._extract_eccn_from_result(result, product_model)
                
                if eccn_info:
                    # è¨ˆç®—ç›¸é—œæ€§å’Œå¯ä¿¡åº¦
                    relevance_score = self._calculate_relevance(result, product_model)
                    credibility_score = self._calculate_credibility(result)
                    
                    processed_result = {
                        **eccn_info,
                        'relevance_score': relevance_score,
                        'credibility_score': credibility_score,
                        'combined_score': (relevance_score + credibility_score) / 2,
                        'source_type': self._classify_source_type(result),
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    processed_results.append(processed_result)
                    
            except Exception as e:
                self.logger.warning(f"çµæœè™•ç†å¤±æ•—: {str(e)}")
                continue
        
        # æŒ‰åˆ†æ•¸æ’åº
        processed_results.sort(key=lambda x: x['combined_score'], reverse=True)
        
        # å»é‡å’Œåˆä½µç›¸ä¼¼çµæœ
        deduplicated_results = self._deduplicate_results(processed_results)
        
        return deduplicated_results

    def _extract_eccn_from_result(self, result: Dict, product_model: str) -> Optional[Dict]:
        """å¾æœç´¢çµæœä¸­æå–ECCNè³‡è¨Š"""
        
        title = result.get('title', '')
        snippet = result.get('snippet', '')
        url = result.get('url', '')
        domain = result.get('domain', '')
        
        combined_text = f"{title} {snippet}".upper()
        
        # å°‹æ‰¾ECCNæ¨¡å¼
        eccn_matches = self.eccn_pattern.findall(combined_text)
        
        if eccn_matches:
            # é¸æ“‡æœ€å¯èƒ½çš„ECCN (æ’é™¤æ˜é¡¯ä¸ç›¸é—œçš„)
            eccn_code = self._select_best_eccn_match(eccn_matches, combined_text)
            
            return {
                'source': 'websearch',
                'eccn_code': eccn_code,
                'url': url,
                'domain': domain,
                'title': title,
                'snippet': snippet,
                'context': self._extract_context(combined_text, eccn_code),
                'confidence': self._assess_eccn_confidence(result, eccn_code, product_model)
            }
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ECCNæ¨¡å¼ï¼Œä½†æ˜¯ä¾†è‡ªæ¬Šå¨ç¶“éŠ·å•†ï¼Œè¨˜éŒ„ç‚ºå¾…æŸ¥è­‰
        if self._is_authoritative_distributor(domain) and self._contains_product_model(combined_text, product_model):
            return {
                'source': 'websearch',
                'eccn_code': 'PENDING_VERIFICATION',  # éœ€è¦é€²ä¸€æ­¥æŸ¥è­‰
                'url': url,
                'domain': domain,
                'title': title,
                'snippet': snippet,
                'context': f"Found on authoritative distributor {domain} - requires page content analysis",
                'confidence': 'medium',
                'requires_page_scraping': True
            }
        
        return None

    def _select_best_eccn_match(self, eccn_matches: List[str], text: str) -> str:
        """å¾å¤šå€‹ECCNåŒ¹é…ä¸­é¸æ“‡æœ€ä½³çš„"""
        
        if len(eccn_matches) == 1:
            return eccn_matches[0].upper()
        
        # å„ªå…ˆç´šæ’åº (æé«˜ 5A992.c çš„å„ªå…ˆç´š)
        eccn_priorities = {
            'EAR99': 1,
            '5A991': 2,
            '5A991.b': 3,
            '5A991.b.1': 4,
            '4A994': 2,
            '5A992.c': 5,  # æœ€é«˜å„ªå…ˆç´š
            '5A992.C': 5   # å¤§å¯«ç‰ˆæœ¬
        }
        
        # æŒ‰å„ªå…ˆç´šå’Œä¸Šä¸‹æ–‡ç›¸é—œæ€§æ’åº
        scored_matches = []
        for eccn in eccn_matches:
            eccn_upper = eccn.upper()
            priority = eccn_priorities.get(eccn_upper, 0)
            context_score = self._calculate_context_relevance(text, eccn_upper)
            
            scored_matches.append((eccn_upper, priority + context_score))
        
        # è¿”å›æœ€é«˜åˆ†çš„ECCN
        scored_matches.sort(key=lambda x: x[1], reverse=True)
        return scored_matches[0][0]

    def _calculate_context_relevance(self, text: str, eccn: str) -> float:
        """è¨ˆç®—ECCNåœ¨æ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡ç›¸é—œæ€§"""
        
        # ç›¸é—œé—œéµå­— (åŠ å¼· 5A992.c çš„è­˜åˆ¥)
        relevance_keywords = {
            'EAR99': ['commercial', 'consumer', 'office', 'standard'],
            '5A991': ['industrial', 'managed', 'ethernet', 'network'],
            '5A991.b': ['security', 'encryption', 'advanced', 'enhanced'],
            '5A991.b.1': ['high-speed', 'gigabit', 'performance', 'fiber'],
            '4A994': ['management', 'monitoring', 'control', 'power'],
            '5A992.c': ['comprehensive', 'high-end', 'advanced', 'managed', 'security', 'performance', 'protocols'],
            '5A992.C': ['comprehensive', 'high-end', 'advanced', 'managed', 'security', 'performance', 'protocols']
        }
        
        keywords = relevance_keywords.get(eccn, [])
        found_keywords = sum(1 for keyword in keywords if keyword in text.lower())
        
        return found_keywords / len(keywords) if keywords else 0

    def _calculate_relevance(self, result: Dict, product_model: str) -> float:
        """è¨ˆç®—æœç´¢çµæœèˆ‡ç”¢å“çš„ç›¸é—œæ€§"""
        
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()
        url = result.get('url', '').lower()
        
        model_clean = product_model.lower().replace('-', '').replace('_', '')
        
        relevance_score = 0.0
        
        # ç”¢å“å‹è™ŸåŒ¹é…
        if model_clean in title.replace('-', '').replace('_', ''):
            relevance_score += 0.4
        elif model_clean in snippet.replace('-', '').replace('_', ''):
            relevance_score += 0.3
        elif model_clean in url.replace('-', '').replace('_', ''):
            relevance_score += 0.2
        
        # ECCNç›¸é—œé—œéµå­—
        eccn_keywords = ['eccn', 'export control', 'classification', 'commerce control list']
        for keyword in eccn_keywords:
            if keyword in title or keyword in snippet:
                relevance_score += 0.1
        
        return min(relevance_score, 1.0)

    def _calculate_credibility(self, result: Dict) -> float:
        """è¨ˆç®—æœç´¢çµæœçš„å¯ä¿¡åº¦"""
        
        domain = result.get('domain', '').lower()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¬Šå¨ä¾†æº
        for auth_domain, info in self.authoritative_domains.items():
            if auth_domain in domain:
                return info['weight'] / 10.0  # æ¨™æº–åŒ–ç‚º0-1
        
        # å…¶ä»–å¯ä¿¡åº¦æŒ‡æ¨™
        credibility_score = 0.5  # åŸºç¤åˆ†æ•¸
        
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()
        
        # æ­£é¢æŒ‡æ¨™
        positive_indicators = ['official', 'specification', 'datasheet', 'technical', 'manufacturer']
        negative_indicators = ['forum', 'discussion', 'blog', 'opinion', 'estimate']
        
        for indicator in positive_indicators:
            if indicator in title or indicator in snippet:
                credibility_score += 0.1
        
        for indicator in negative_indicators:
            if indicator in title or indicator in snippet:
                credibility_score -= 0.1
        
        return max(0.0, min(credibility_score, 1.0))

    def _classify_source_type(self, result: Dict) -> str:
        """åˆ†é¡ä¾†æºé¡å‹"""
        
        domain = result.get('domain', '').lower()
        
        for auth_domain, info in self.authoritative_domains.items():
            if auth_domain in domain:
                return info['type']
        
        # æ ¹æ“šåŸŸåç‰¹å¾µåˆ†é¡
        if any(gov_tld in domain for gov_tld in ['.gov', '.mil']):
            return 'government'
        elif any(edu_tld in domain for edu_tld in ['.edu', '.ac.']):
            return 'academic'
        elif 'export' in domain or 'compliance' in domain:
            return 'compliance'
        else:
            return 'general'

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡ç›¸ä¼¼çš„æœç´¢çµæœ"""
        
        unique_results = []
        seen_eccns = set()
        
        for result in results:
            eccn = result.get('eccn_code', '')
            url = result.get('url', '')
            
            # çµ„åˆå”¯ä¸€éµ
            unique_key = f"{eccn}_{self._extract_domain(url)}"
            
            if unique_key not in seen_eccns:
                seen_eccns.add(unique_key)
                unique_results.append(result)
        
        return unique_results

    def _extract_domain(self, url: str) -> str:
        """å¾URLæå–åŸŸå"""
        
        import urllib.parse
        try:
            parsed = urllib.parse.urlparse(url)
            return parsed.netloc.lower()
        except:
            return ''

    def _extract_context(self, text: str, eccn: str) -> str:
        """æå–ECCNå‘¨åœçš„ä¸Šä¸‹æ–‡"""
        
        eccn_pos = text.upper().find(eccn.upper())
        if eccn_pos == -1:
            return ''
        
        # æå–å‰å¾Œ50å€‹å­—ç¬¦
        start = max(0, eccn_pos - 50)
        end = min(len(text), eccn_pos + len(eccn) + 50)
        
        return text[start:end].strip()

    def _assess_eccn_confidence(self, result: Dict, eccn: str, product_model: str) -> str:
        """è©•ä¼°ECCNè³‡è¨Šçš„ä¿¡å¿ƒåº¦"""
        
        relevance = self._calculate_relevance(result, product_model)
        credibility = self._calculate_credibility(result)
        
        combined_score = (relevance + credibility) / 2
        
        if combined_score >= 0.8:
            return 'high'
        elif combined_score >= 0.6:
            return 'medium'
        else:
            return 'low'

    def _infer_manufacturer_domain(self, manufacturer: str) -> Optional[str]:
        """æ¨æ–·è£½é€ å•†çš„ç¶²åŸŸåç¨±"""
        
        manufacturer_mapping = {
            'advantech': 'advantech.com',
            'cisco': 'cisco.com',
            'dell': 'dell.com',
            'hp': 'hp.com',
            'intel': 'intel.com',
            'siemens': 'siemens.com',
            'schneider': 'schneider-electric.com',
            'moxa': 'moxa.com',
            'phoenix': 'phoenixcontact.com'
        }
        
        manufacturer_lower = manufacturer.lower()
        
        for key, domain in manufacturer_mapping.items():
            if key in manufacturer_lower:
                return domain
        
        # å˜—è©¦æ§‹å»ºåŸŸå
        clean_name = re.sub(r'[^a-zA-Z]', '', manufacturer_lower)
        if len(clean_name) > 2:
            return f"{clean_name}.com"
        
        return None

    def _is_authoritative_distributor(self, domain: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ¬Šå¨ç¶“éŠ·å•†åŸŸå"""
        authoritative_distributors = [
            'mouser.com', 'digikey.com', 'arrow.com', 'farnell.com',
            'element14.com', 'rs-online.com', 'newark.com'
        ]
        
        return any(dist in domain.lower() for dist in authoritative_distributors)

    def _contains_product_model(self, text: str, product_model: str) -> bool:
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ç”¢å“å‹è™Ÿ"""
        text_clean = text.replace('-', '').replace('_', '').upper()
        model_clean = product_model.replace('-', '').replace('_', '').upper()
        
        return model_clean in text_clean

    def _duckduckgo_search(self, query: str) -> List[Dict]:
        """ä½¿ç”¨DuckDuckGoé€²è¡ŒçœŸå¯¦ç¶²è·¯æœç´¢"""
        
        try:
            import urllib.parse
            from bs4 import BeautifulSoup
            
            # DuckDuckGoæœç´¢URL
            encoded_query = urllib.parse.quote_plus(query)
            search_url = f"https://html.duckduckgo.com/html/?q={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            self.logger.info(f"ğŸ” DuckDuckGoæœç´¢: {query}")
            
            response = requests.get(search_url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç´¢çµæœ - ä¿®å¾©DuckDuckGo HTMLçµæ§‹è§£æ
            for result_div in soup.find_all('div', class_='result'):
                try:
                    # æ‰¾ç¬¬ä¸€å€‹æœ‰æ•ˆé€£çµä½œç‚ºä¸»è¦çµæœ
                    title_element = None
                    for link in result_div.find_all('a'):
                        if link.get('href') and link.get_text(strip=True):
                            title_element = link
                            break
                    
                    if not title_element:
                        continue
                        
                    title = title_element.get_text(strip=True)
                    url = title_element.get('href', '')
                    
                    # è™•ç†DuckDuckGoé‡å®šå‘URL
                    if url.startswith('//duckduckgo.com/l/?uddg='):
                        # è§£ç¢¼é‡å®šå‘URL
                        url = urllib.parse.unquote(url.split('uddg=')[1])
                    elif url.startswith('//'):
                        url = 'https:' + url
                    
                    # æå–æ‘˜è¦ - å¾result__bodyä¸­æŸ¥æ‰¾æ–‡æœ¬
                    snippet = ''
                    body_element = result_div.find('div', class_='result__body')
                    if body_element:
                        # å–å¾—æ‰€æœ‰æ–‡æœ¬ä½†æ’é™¤é€£çµæ–‡å­—
                        all_text = body_element.get_text(separator=' ', strip=True)
                        snippet = all_text.replace(title, '').strip()[:200]
                    
                    # æå–åŸŸå
                    domain = self._extract_domain(url)
                    
                    if title and url and domain:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'domain': domain
                        })
                        
                        if len(results) >= self.max_results_per_query:
                            break
                            
                except Exception as e:
                    self.logger.warning(f"è§£ææœç´¢çµæœå¤±æ•—: {str(e)}")
                    continue
            
            self.logger.info(f"âœ… DuckDuckGoæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} å€‹çµæœ")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ DuckDuckGoæœç´¢å¤±æ•—: {str(e)}")
            return []

    def _google_custom_search(self, query: str) -> List[Dict]:
        """Google Custom Search APIå¯¦ç¾ (éœ€è¦APIé‡‘é‘°)"""
        
        # é€™è£¡å¯ä»¥å¯¦ç¾Google Custom Search API
        # éœ€è¦GOOGLE_API_KEYå’ŒGOOGLE_CSE_ID
        self.logger.warning("Google Custom Searchæœªå¯¦ç¾ï¼Œéœ€è¦APIé‡‘é‘°")
        return []

    def _bing_search(self, query: str) -> List[Dict]:
        """Bing Search APIå¯¦ç¾ (éœ€è¦APIé‡‘é‘°)"""
        
        # é€™è£¡å¯ä»¥å¯¦ç¾Bing Search API
        # éœ€è¦BING_SUBSCRIPTION_KEY
        self.logger.warning("Bing Searchæœªå¯¦ç¾ï¼Œéœ€è¦APIé‡‘é‘°")
        return []

# ä½¿ç”¨ç¤ºä¾‹
def test_websearch():
    """æ¸¬è©¦WebSearchåŠŸèƒ½"""
    
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    searcher = ECCNWebSearcher(logger)
    
    test_products = [
        ("EKI-2428G-4CA-AE", "Advantech"),
        ("EKI-5525I-AE", "Advantech"),
        ("TN-5510A-2L", "Moxa")
    ]
    
    print("ğŸ” WebSearch æ¸¬è©¦")
    print("=" * 50)
    
    for model, manufacturer in test_products:
        print(f"\nğŸ” æœç´¢: {model} ({manufacturer})")
        
        results = searcher.search_eccn_information(model, manufacturer)
        
        print(f"æ‰¾åˆ° {len(results)} å€‹çµæœ:")
        for i, result in enumerate(results[:3], 1):  # é¡¯ç¤ºå‰3å€‹çµæœ
            print(f"  {i}. ECCN: {result.get('eccn_code', 'N/A')}")
            print(f"     ä¿¡å¿ƒåº¦: {result.get('confidence', 'unknown')}")
            print(f"     ä¾†æº: {result.get('domain', 'unknown')}")
            print(f"     åˆ†æ•¸: {result.get('combined_score', 0):.2f}")

if __name__ == "__main__":
    test_websearch()